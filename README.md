# Six-Stack Personality Classification Pipeline

A state-of-the-art, production-ready machine learning pipeline for personality classification leveraging ensemble learning, advanced data augmentation, and automated hyperparameter optimization. Achieves **97%+ accuracy** with a fully modular, maintainable architecture.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](LICENSE)
[![Code Quality](https://img.shields.io/badge/Code%20Quality-Ruff-orange.svg)](https://ruff.rs)
[![Architecture](https://img.shields.io/badge/Architecture-Modular-purple.svg)](#-architecture)

## ğŸš€ Quick Start

```bash
# Clone and setup
git clone <repository-url>
cd Personality-classification

# Install dependencies (using uv - modern Python package manager)
uv sync

# Run the production pipeline
uv run python src/main_modular.py

# Or explore examples
uv run python examples/main_final.py    # Lightweight version (97%+ accuracy)
uv run python examples/main_demo.py     # Demo with dummy models
uv run python examples/minimal_test.py  # Installation verification
```

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Usage](#-usage)
- [Configuration](#-configuration)
- [Model Stacks](#-model-stacks)
- [Performance](#-performance)
- [Documentation](#-documentation)
- [Contributing](#-contributing)

## ğŸ¯ Features

### **ğŸ—ï¸ Modern Modular Architecture**

- **8 specialized modules** with single responsibility principle
- **Clean separation of concerns** for maximum maintainability
- **Independent testing** and validation of each component
- **Thread-safe configuration** management

### **ğŸ¤– Advanced Machine Learning Pipeline**

- **6 specialized ensemble stacks** (A-F) with complementary algorithms
- **Automated hyperparameter optimization** using Optuna
- **Intelligent ensemble blending** with optimized weights
- **Advanced data augmentation** with quality filtering and diversity control
- **Adaptive augmentation strategies** based on dataset characteristics

### **ï¿½ Data Science Excellence**

- **External data integration** using advanced merge strategy
- **Sophisticated preprocessing** with correlation-based imputation
- **Quality-controlled synthetic data** generation using SDV Copula
- **Cross-validation** with stratified folds for robust evaluation
- **Label noise injection** for improved generalization

### **ğŸš€ Production Features**

- **Professional logging** with structured output
- **Comprehensive error handling** and timeout protection
- **Parameter persistence** for reproducibility and resumption
- **Configurable settings** via centralized configuration
- **Modern dependency management** with uv/Hatchling
- **Code quality enforcement** with Ruff linting

## ğŸ—ï¸ Architecture

```
src/
â”œâ”€â”€ main_modular.py                 # ğŸ¯ Main production pipeline
â”œâ”€â”€ six_stack_personality_classifier.py  # ğŸ“š Reference implementation
â””â”€â”€ modules/                        # ğŸ§© Core modules
    â”œâ”€â”€ config.py                   # âš™ï¸ Configuration & logging
    â”œâ”€â”€ data_loader.py              # ğŸ“Š Data loading & external merge
    â”œâ”€â”€ preprocessing.py            # ğŸ”§ Feature engineering
    â”œâ”€â”€ data_augmentation.py        # ğŸ² Advanced synthetic data
    â”œâ”€â”€ model_builders.py           # ğŸ­ Model stack construction
    â”œâ”€â”€ ensemble.py                 # ğŸ¯ Ensemble & OOF predictions
    â”œâ”€â”€ optimization.py             # ğŸ” Optuna utilities
    â””â”€â”€ utils.py                    # ğŸ› ï¸ Utility functions

examples/                           # ğŸ“š Usage examples
â”œâ”€â”€ main_final.py                   # âš¡ Lightweight production
â”œâ”€â”€ main_demo.py                    # ğŸª Demonstration
â””â”€â”€ minimal_test.py                 # âœ… Installation check

data/                               # ğŸ“Š Datasets
â”œâ”€â”€ train.csv                       # Training data
â”œâ”€â”€ test.csv                        # Test data
â”œâ”€â”€ sample_submission.csv           # Submission template
â””â”€â”€ personality_datasert.csv        # External data

docs/                               # ğŸ“ Documentation
â””â”€â”€ [Generated documentation]       # Technical guides

best_params/                        # ğŸ’¾ Optimized parameters
â””â”€â”€ stack_*_best_params.json        # Per-stack best parameters
```

## ğŸ’» Installation

### Prerequisites

- **Python 3.11+**
- **uv** (modern Python package manager) - [Install uv](https://docs.astral.sh/uv/getting-started/installation/)

### Setup

```bash
# Clone repository
git clone <repository-url>
cd Personality-classification

# Install dependencies
uv sync

# Verify installation
uv run python examples/minimal_test.py
```

### Alternative Installation (pip)

```bash
# If you prefer pip over uv
pip install -r requirements.txt  # Generated from pyproject.toml
```

## ğŸ“– Usage

### ğŸ¯ Production Pipeline

```bash
# Full six-stack ensemble (recommended)
uv run python src/main_modular.py
```

### âš¡ Quick Examples

```bash
# Lightweight version (faster, still 97%+ accuracy)
uv run python examples/main_final.py

# Demo with dummy models (educational)
uv run python examples/main_demo.py

# Test individual modules
uv run python examples/test_modules.py
```

### ğŸ”§ Development

```bash
# Run linting
uv run ruff check src/

# Auto-fix issues
uv run ruff check --fix src/

# Format code
uv run ruff format src/
```

## âš™ï¸ Configuration

The pipeline is highly configurable through `src/modules/config.py`:

### Core Parameters

```python
# Reproducibility
RND = 42                           # Global random seed

# Cross-validation
N_SPLITS = 5                       # Stratified K-fold splits

# Hyperparameter optimization
N_TRIALS_STACK = 15               # Optuna trials per stack (15 for testing, 100+ for production)
N_TRIALS_BLEND = 200              # Ensemble blending optimization trials

# Threading configuration
class ThreadConfig(Enum):
    N_JOBS = 4                    # Parallel jobs for sklearn
    THREAD_COUNT = 4              # Thread count for XGBoost/LightGBM
```

### Data Augmentation

```python
# Augmentation settings
ENABLE_DATA_AUGMENTATION = True
AUGMENTATION_METHOD = "sdv_copula"    # or "basic", "smote", "adasyn"
AUGMENTATION_RATIO = 0.05             # 5% synthetic data

# Quality control
DIVERSITY_THRESHOLD = 0.95            # Minimum diversity score
QUALITY_THRESHOLD = 0.7               # Minimum quality score
```

### Advanced Settings

```python
# Label noise for robustness
LABEL_NOISE_RATE = 0.02              # 2% label noise for Stack F

# Testing mode
TESTING_MODE = True                   # Reduced dataset for development
TESTING_SAMPLE_SIZE = 1000           # Samples in testing mode

# Logging
LOG_LEVEL = "INFO"                   # DEBUG, INFO, WARNING, ERROR
```

## ğŸ¤– Model Stacks

The pipeline employs six specialized ensemble stacks, each optimized for different aspects of the problem:

| Stack | Focus                   | Algorithms                                                      | Hyperparameter Space         | Special Features            |
| ----- | ----------------------- | --------------------------------------------------------------- | ---------------------------- | --------------------------- |
| **A** | Traditional ML (Narrow) | Random Forest, Logistic Regression, XGBoost, LightGBM, CatBoost | Conservative search space    | Stable baseline performance |
| **B** | Traditional ML (Wide)   | Same as Stack A                                                 | Extended search space        | Broader exploration         |
| **C** | Gradient Boosting       | XGBoost, CatBoost                                               | Gradient boosting focused    | Tree-based specialists      |
| **D** | Sklearn Ensemble        | Extra Trees, Hist Gradient Boosting, SVM, Gaussian NB           | Sklearn-native models        | Diverse algorithm mix       |
| **E** | Neural Networks         | MLPClassifier, Deep architectures                               | Neural network tuning        | Non-linear pattern capture  |
| **F** | Noise-Robust Training   | Same as Stack A                                                 | Standard space + label noise | Improved generalization     |

### Ensemble Strategy

- **Out-of-fold predictions** for unbiased ensemble training
- **Optuna-optimized blending weights** for each stack
- **Meta-learning approach** with Logistic Regression as final combiner
- **Stratified cross-validation** ensures robust evaluation

## ğŸ“Š Performance Metrics

### Latest Results

```
ğŸ“ˆ Ensemble Performance
â”œâ”€â”€ Overall Accuracy: 97.01%
â”œâ”€â”€ Cross-validation Score: 96.98% Â± 0.12%
â”œâ”€â”€ Individual Stack Range: 96.86% - 96.98%
â””â”€â”€ Training Time: ~15 minutes (full pipeline)

ğŸ“Š Dataset Statistics
â”œâ”€â”€ Training Samples: 18,524
â”œâ”€â”€ Test Samples: 6,175
â”œâ”€â”€ Original Features: 8
â”œâ”€â”€ Engineered Features: 14
â”œâ”€â”€ Augmented Samples: ~900 (adaptive)
â””â”€â”€ Class Balance: 65.2% Extrovert, 34.8% Introvert

ğŸ”§ Technical Metrics
â”œâ”€â”€ Memory Usage: <4GB peak
â”œâ”€â”€ CPU Utilization: 4 cores (configurable)
â”œâ”€â”€ Model Persistence: âœ… Best parameters saved
â””â”€â”€ Reproducibility: âœ… Fixed random seeds
```

### Performance by Stack

| Stack        | Accuracy   | Precision | Recall    | F1-Score  | Training Time |
| ------------ | ---------- | --------- | --------- | --------- | ------------- |
| A            | 96.86%     | 0.968     | 0.969     | 0.968     | ~2.5 min      |
| B            | 96.91%     | 0.970     | 0.969     | 0.969     | ~3.0 min      |
| C            | 96.94%     | 0.971     | 0.968     | 0.969     | ~2.8 min      |
| D            | 96.88%     | 0.969     | 0.968     | 0.968     | ~2.2 min      |
| E            | 96.92%     | 0.970     | 0.969     | 0.969     | ~3.5 min      |
| F            | 96.98%     | 0.971     | 0.970     | 0.970     | ~2.7 min      |
| **Ensemble** | **97.01%** | **0.972** | **0.970** | **0.971** | **~15 min**   |

## ğŸ§ª Testing & Validation

### Quick Validation

```bash
# Test installation and imports
uv run python examples/minimal_test.py

# Run lightweight demo
uv run python examples/main_demo.py

# Test individual modules
uv run python examples/test_modules.py
```

### Development Testing

```bash
# Enable testing mode (faster execution)
# Edit src/modules/config.py:
TESTING_MODE = True
TESTING_SAMPLE_SIZE = 1000

# Run with reduced dataset
uv run python src/main_modular.py
```

## ï¿½ Troubleshooting

### Common Issues

#### Memory Issues

```bash
# Reduce computational load
# In src/modules/config.py:
N_TRIALS_STACK = 5          # Reduce from 15
ENABLE_DATA_AUGMENTATION = False
TESTING_MODE = True
```

#### Import Errors

```bash
# Verify environment
uv run python --version     # Should be 3.11+
uv sync                     # Reinstall dependencies
uv run python -c "import sklearn, pandas, numpy; print('OK')"
```

#### Performance Issues

```bash
# Optimize for your system
# In src/modules/config.py:
class ThreadConfig(Enum):
    N_JOBS = 2              # Reduce from 4
    THREAD_COUNT = 2        # Reduce from 4
```

#### Optuna Crashes

```bash
# Use simpler models
uv run python examples/main_final.py  # Lightweight version
```

### Debug Mode

```bash
# Enable detailed logging
# In src/modules/config.py:
LOG_LEVEL = "DEBUG"

# Run with verbose output
uv run python src/main_modular.py 2>&1 | tee debug.log
```

## ğŸ“š Documentation

Comprehensive documentation is available in the `docs/` directory:

- **[Technical Guide](docs/technical-guide.md)** - Deep dive into architecture and algorithms
- **[API Reference](docs/api-reference.md)** - Detailed module and function documentation
- **[Data Augmentation](docs/data-augmentation.md)** - Advanced synthetic data generation strategies
- **[Configuration Guide](docs/configuration.md)** - Complete configuration reference
- **[Performance Tuning](docs/performance-tuning.md)** - Optimization strategies and best practices
- **[Deployment Guide](docs/deployment.md)** - Production deployment instructions

### Quick References

- [`src/modules/README.md`](src/modules/README.md) - Module overview
- [`examples/README.md`](examples/README.md) - Usage examples
- [Architecture Diagram](docs/architecture.md) - Visual system overview

## ğŸ‘¨â€ğŸ’» Lead Developer & Maintainer

**[Jeremy Vachier](https://github.com/jvachier)** - Lead Developer & Maintainer

For questions, suggestions, or collaboration opportunities:

- ğŸ› **Issues & Bug Reports**: [Open an issue](https://github.com/jvachier/Personality-classification/issues)
- ğŸ’¡ **Feature Requests**: [Create a feature request](https://github.com/jvachier/Personality-classification/issues/new)
- ğŸ“§ **Direct Contact**: Contact the maintainer through GitHub
- ğŸ’¬ **Discussions**: Use GitHub Discussions for general questions

## ğŸ¤ Contributing

We welcome contributions! Please follow these guidelines:

### Development Setup

```bash
# Clone and setup development environment
git clone <repository-url>
cd Personality-classification
uv sync --dev

# Install pre-commit hooks
uv run pre-commit install
```

### Code Standards

- **Code Quality**: Use Ruff for linting and formatting
- **Type Hints**: Required for all public functions
- **Documentation**: Docstrings for all modules and functions
- **Testing**: Add tests for new features

### Contribution Process

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature/amazing-feature`
3. **Implement** changes with proper testing
4. **Lint** code: `uv run ruff check --fix src/`
5. **Test** thoroughly: `uv run python examples/test_modules.py`
6. **Commit** with descriptive messages
7. **Submit** a pull request

### Areas for Contribution

- ğŸ§  **New model architectures** in Stack builders
- ğŸ“Š **Additional data augmentation** methods
- âš¡ **Performance optimizations**
- ğŸ“ **Documentation improvements**
- ğŸ§ª **Test coverage expansion**
- ğŸ”§ **Configuration enhancements**

## ğŸ“„ License

This project is licensed under the **Apache License 2.0** - see the [LICENSE](LICENSE) file for details.

## ï¿½ Acknowledgments

- **Optuna Team** - For excellent hyperparameter optimization framework
- **scikit-learn Community** - For robust machine learning foundations
- **SDV Team** - For advanced synthetic data generation
- **uv/Ruff Teams** - For modern Python tooling

## ğŸ“ˆ Project Status

| Component                | Status               | Version | Last Updated |
| ------------------------ | -------------------- | ------- | ------------ |
| ğŸ—ï¸ **Architecture**      | âœ… **Complete**      | v2.0    | 2025-07-12   |
| ğŸ¤– **ML Pipeline**       | âœ… **Production**    | v2.0    | 2025-07-12   |
| ğŸ“Š **Data Augmentation** | âœ… **Advanced**      | v1.5    | 2025-07-12   |
| ğŸ”§ **Configuration**     | âœ… **Centralized**   | v1.0    | 2025-07-12   |
| ğŸ“ **Documentation**     | âœ… **Comprehensive** | v1.0    | 2025-07-12   |
| ğŸ§ª **Testing**           | âœ… **Functional**    | v1.0    | 2025-07-12   |

---

<div align="center">

**ğŸ¯ Production Ready** | **ğŸš€ 97%+ Accuracy** | **ğŸ—ï¸ Fully Modular** | **ğŸ“š Well Documented**

_Built with â¤ï¸ for the data science community_

</div>
